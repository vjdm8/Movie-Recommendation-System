{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "name": "",
  "signature": "sha256:670b7c6747a07131add5b6ba720258d542988385862cb6c03fbc7aadda671fc4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Import recommendation from pyspark's MlLib"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Read ratings.dat file as a text file"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = sc.textFile(\"file:///home/cloudera/Downloads/ml-10M100K/ratings.dat\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Verify Data is read correctly"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data.count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "10000054"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Explore the data format (In order to parse)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data.take(3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "['1::122::5::838985046', '1::185::5::838983525', '1::231::5::838983392']"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Transform and Load data as a spark dataframe"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Parse the data by splitting by the '::' delimiter\n",
      "#Utilize the Rating function to structure the data\n",
      "#Eliminate timestamp (unneccessary information)\n",
      "ratings = data.map(lambda l: l.split('::'))\\\n",
      "    .map(lambda l: Rating(int(l[0]), int(l[1]), float(l[2])))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Peek at created data frame"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ratings.take(3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "[Rating(user=1, product=122, rating=5.0),\n",
        " Rating(user=1, product=185, rating=5.0),\n",
        " Rating(user=1, product=231, rating=5.0)]"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Split Data into Test & Train"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "training_RDD, validation_RDD = ratings.randomSplit([7, 3], seed=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from pyspark.mllib.recommendation import ALS\n",
      "import math"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Remove the Y-Variable from Test"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "#saving no y-var seperately\n",
      "validation_for_predict_RDD = validation_RDD.map(lambda x: (x[0], x[1]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Running the ALS Recommender algorithm"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "t_start=time.clock() #start counting process time\n",
      "seed = 5\n",
      "iterations = 10\n",
      "regularization_parameter = 0.1\n",
      "ranks = [4, 8, 12] #ranks for matrix factorization\n",
      "errors = [0, 0, 0]\n",
      "err = 0\n",
      "tolerance = 0.02\n",
      "\n",
      "min_error = float('inf')\n",
      "best_rank = -1 #intialize\n",
      "best_iteration = -1 #intialize\n",
      "\n",
      "#run model for each rank\n",
      "for rank in ranks:\n",
      "    #train model\n",
      "    model = ALS.train(training_RDD, rank, seed=seed, iterations=iterations,lambda_=regularization_parameter)\n",
      "    #use trained model to predict\n",
      "    predictions = model.predictAll(validation_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
      "    #join predicted rating to dataset containing actual rating\n",
      "    rates_and_preds = validation_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
      "    #calculate RMSE\n",
      "    error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
      "    #save error for current rank into list of errors\n",
      "    errors[err] = error\n",
      "    #increment iterator\n",
      "    err += 1\n",
      "    print ('For rank %s the RMSE is %s' % (rank, error))\n",
      "    #save the smallest error and its corresponding best rank to print later\n",
      "    if error < min_error:\n",
      "        min_error = error\n",
      "        best_rank = rank\n",
      "t_end=time.clock() #stop timing process\n",
      "print ('The best model was trained with rank %s' % best_rank)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "For rank 4 the RMSE is 0.8255221410787698\n",
        "For rank 8 the RMSE is 0.8205743044041366\n",
        "For rank 12 the RMSE is 0.8196217015098444\n",
        "The best model was trained with rank 12\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Calculate CPU time to observe adjusment to scale"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "CPUt=t_end-t_start #difference between start and end time\n",
      "print ('CPU time is %s' %(CPUt))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU time is 0.5800000000000001\n"
       ]
      }
     ],
     "prompt_number": 11
    }
   ],
   "metadata": {}
  }
 ]
}